# -*- coding: utf-8 -*-
"""Named_Entity_Recognition_ELMo_copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZmgcFcSwmt8fOi-NM63fWUSj_ThEd-nM
"""

import spacy
import random
import json
from zipfile import ZipFile
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use("ggplot")
# nltk.download('punkt')
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.models import Model, Input
from tensorflow.python.keras.layers.merge import add
from tensorflow.python.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda
from sklearn.metrics import precision_score, recall_score, f1_score, classification_report


# please install tf version 2.1 using the following command
#!pip install tensorflow==2.1

# please install keras version 1.2 using the following command

# !pip install q keras==1.2.2
# print(tf.keras.__version__)


from zipfile import ZipFile
with ZipFile('/actions.zip', 'r') as zf:
    # zf.printdir()
    #Extracting the files from zip file
    zf.extractall()
    print('Zip Extraction Completed')


ctr=0
final={}
final[ctr]=[]
for i in range(2,100):
    fnum='{0:04}'.format(i)
    print(fnum)
    try:
      with open('/content/actions/'+fnum+'.json') as f:
        data=f.readlines()
      f.close()
    except:
      continue

    if type(data)==list:
      data=json.loads(data[0])
    try:
      data.pop('id')
    except:
      pass

    p=0
    tok=word_tokenize(data['text'])
    while p<len(tok):
       while tok[p]!='.':
          sind=data['text'].find(tok[p])
          f=0
          for l in data['labels']:
            if l[0] == sind:
              f=1
              final[ctr].append((tok[p],'ACTIONS'))
              break
              
          if f == 0:
            final[ctr].append((tok[p],'OTHER'))
          p=p+1
      
       ctr=ctr+1
       p=p+1
       final[ctr]=[]



print('length of dataset:',len(final))
max=0
for i in final.keys():
  if len(final[i])>max:
    max=len(final[i])
print('max length of sentencein dataset:',max)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.hist([len(final[i]) for i in final.keys()], bins= 50)
plt.show()

words=set()
for i in final.values():
  for x in i:
    words.add(x[0])
words.add('PADword')
n_words = len(words)
print('vocab size:',n_words)

tags=set()
for i in final.values():
  for x in i:
    tags.add(x[1])
n_tags = len(tags)
print('no of tags:',n_tags)

words2index = {w:i for i,w in enumerate(words)}
tags2index = {t:i for i,t in enumerate(tags)}
# print(words2index['used'])
# print(tags2index['OTHER'])

# setting max length of input vector to 50
max_len=50
X=[[x[0] for x in i]for i in final.values()]
len(X)
new_X=[]
for seq in X:
  new_seq=[]
  for i in range(max_len):
    try:
      new_seq.append(seq[i])
    except:
      new_seq.append('PADword')
  new_X.append(new_seq)
# len(new_X[3])


y=[[tags2index[w[1]] for w in i]for i in final.values()]
y = pad_sequences(maxlen=max_len, sequences=y, padding="post", value=tags2index["OTHER"])
# y[15],len(y)

# train test split
X_tr, X_te, y_tr, y_te = train_test_split(new_X, y, test_size=0.1, random_state=2018)
len(X_tr),len(X_te),len(y_tr),len(y_te)

# disabling eager executiomg
tf.compat.v1.disable_eager_execution()

# setting batch size
batch_size = 32
sess=tf.compat.v1.Session()
tf.compat.v1.keras.backend.set_session(sess)


elmo_model = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)
sess.run(tf.compat.v1.global_variables_initializer())
sess.run(tf.compat.v1.tables_initializer())

# defin embedding function
def ElmoEmbedding(x):
    return elmo_model(inputs={
                            "tokens": tf.squeeze(tf.cast(x, tf.string)),
                            "sequence_len": tf.constant(batch_size*[max_len])
                      },
                      signature="tokens",
                      as_dict=True)["elmo"]


# define model
input_text = Input(shape=(max_len,), dtype=tf.string)
embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)
x = Bidirectional(LSTM(units=512, return_sequences=True,
                       recurrent_dropout=0.2, dropout=0.2))(embedding)
x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,
                           recurrent_dropout=0.2, dropout=0.2))(x)
x = add([x, x_rnn])  # residual connection to the first biLSTM
out = TimeDistributed(Dense(n_tags, activation="softmax"))(x)

model = Model(input_text, out)
model.compile(optimizer="adam", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))

# define validation sets
X_tr, X_val = X_tr[:1213*batch_size], X_tr[-135*batch_size:]
y_tr, y_val = y_tr[:1213*batch_size], y_tr[-135*batch_size:]
y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)
y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)


# training 
history = model.fit(np.array(X_tr), np.array(y_tr), validation_data=(np.array(X_val), np.array(y_val)),
                    batch_size=batch_size, epochs=3, verbose=1)

# testing
X_te = X_te[:149*batch_size]
test_pred = model.predict(np.array(X_te), verbose=1)

idx2tag = {i: w for w, i in tags2index.items()}


def pred2label(pred):
    out = []
    for pred_i in pred:
        out_i = []
        for p in pred_i:
            p_i = np.argmax(p)
            out_i.append(idx2tag[p_i].replace("PADword", "O"))
        out.append(out_i)
    return out

def test2label(pred):
    out = []
    for pred_i in pred:
        out_i = []
        for p in pred_i:
            out_i.append(idx2tag[p].replace("PADword", "O"))
        out.append(out_i)
    return out
    
pred_labels = pred2label(test_pred)
test_labels = test2label(y_te[:149*32])

print("F1-score: {:.1%}".format(f1_score(test_labels, pred_labels)))

print(classification_report(test_labels, pred_labels))

i = 390
p = model.predict(np.array(X_te[i:i+batch_size]))[0]
p = np.argmax(p, axis=-1)
print("{:15} {:5}: ({})".format("Word", "Pred", "True"))
print("="*30)
for w, true, pred in zip(X_te[i], y_te[i], p):
    if w != "PADword":
        print("{:15}:{:5} ({})".format(w, tags[pred], tags[true]))

